/**********************************************************
* Project 4: gerp
* CS 15: Data Structures
* README
* Authors: Holden Kittelberger & Dan Glorioso (hkittel01 & dglori02)
* Date: 11/30/2023
*
*********************************************************/

B. Program Purpose:

     The purpose of this program is to create a search tool that allows the
     user to search for words in a directory of files. The user can search for
     a word, either case sensitive or case insensitive, and the program will
     return the line number and file path where the word was found. The user
     can also search for a word with a specific variation of capitalization
     using the command "@i" or “@insensitive” followed by a string. The program 
     also allows a user to redirect the output of a search by using the "@f" 
     query command. The program is quit by using the query command “@q” or 
     “@quit”. 
 
C. Acknowledgements:
     We worked on this project independently. For reference of our data 
     structures and libraries used, we used the following resources on the 
     C++ website: 
     https://cplusplus.com/reference/string/string/substr/. 
     https://cplusplus.com/reference/functional/hash/, 
     https://cplusplus.com/reference/cctype/isalnum/. 

D. Files:

main.cpp:
     Main function for the Gerp program. This program is a search tool that
     allows the user to search for words in a directory of files. The user can
     search for a word, either case sensitive or case insensitive, and the
     program will return the line number and file name where the word was
     found.

Gerp.cpp:
     Contains Implementation of the Gerp class which creates a tree of the
     input directory, traverses the tree, and indexes each file in the tree.
     Also runs the query loop and executes commands. Functions include
     makeTree, traverseTree, readLines, parseLine, stripNonAlphaNum,
     open_or_die, abort, Query, prompt_and_execute, and printLocations.

Gerp.h:
     Declaration of the Gerp class which will be used to index and search
     through files in a directory tree and run the program query loop.
     Functions include a constructor, destructor, and functions to index the
     files in a directory tree and take in user queries.

HashTable.cpp:
     Implementation of HashTable class which is a hash table that stores words
     and their locations. The hash table is dynamically resized when the load
     factor exceeds 0.7. The hashVector is a vector of linked lists that stores
     Value structs. The Value struct stores the key (word), a set of wordVar
     structs, and a set of wordLocations. The wordVar struct stores the word
     variations (capitalization) and a set of wordLocations. The wordLocation
     struct stores the line number and file index of the word.

HashTable.h:
     Declaration of the HashTable class. This class is used to store all of the
     words and their locations in the files. It is used to search for words and
     their locations in the files. Functions include: insertWord, getBucket,
     makeLower, createValHelper, createValue, createWordVar,
     createWordLocation, traverseLL, traverseWordVars, getLoadFactor, expand,
     expandHelper, and print_ValStruct.

File.cpp:
     Implementation of the File class that stores the name of the file and the
     path to the file as well as a vector of string representing the lines of
     the file. Functions include getters and setters for the file name and
     path, a function to add a line to the vector of lines, a function to print
     the lines, and a function to return the number of lines in the file.

File.h:
     Implementation of the File class which represents a file in the FSTree.
     Each file contains a vector of strings which represent the lines of the
     file. The File class contains getters and setters for the file name, path,
     and lines. It also contains a function to add a line to the vector of
     lines and a function to print the lines.

FSTreeTraversal.cpp:
     Contains implementation of the function that traverse the FSTree by
     entering an initial directory and traverse through its subdirectories
     while printing out all of its file contents along the way with their
     paths.

FSTree.h:
     Interface of the FSTree class that contains an n-ary tree to represent
     a file system

DirNode.h:
     Interface of the DirNode class that contains a node for the FSTree class
     that contains a vector of DirNode pointers to represent the children of
     the node and a pointer to the parent of the node.

unit_test.h:
     Framework to test the functionality of the stringProcessing and gerp
     class functions.

Makefile:
     Makefile compiles the program with the command "make gerp"

x.cpp
     File for the Foo directory in the spec

y.cpp
     File for the Foo directory in the spec

z.cpp
     File for the Foo directory in the spec

a.cpp
     File for the Foo directory in the spec

b.cpp
     File for the Foo directory in the spec

c.cpp
     File for the Foo directory in the spec

hello.txt
     Txt file for index a directory of a single file

test.txt
     Txt file for an index of a directory of 2 files

test2.txt
     Txt file for an index of a directory of 2 files

alphaInput.txt
     Input file with only alphanumeric character

alphanumeric.txt
     Indexing file of only non-alphanumeric character

diffout.txt
     Output for the reference of the_gerp

output.txt
     Output for our implementation of the gerp

newoutput.txt
     Output file for when @f is used and a new output file is needed

inputNoQuit.txt
     Input file with no quit

mediumInput.txt
     Largest input file with many lines of input for a stress test of our 
     program

singleInput.txt
     Only a single search

file_change.txt
     Input file that has the @f command

many_file_changes.txt
     Input file with multiple @f commands

multi_input.txt
     Input with many words to query all on the same line right after each other

insen.txt
     Input that uses exclusively insensitive searches

mixed.txt
     Input that uses insensitive and sensitive searches

notfound.txt
     Input that won’t be found

README:
     This file.


E. Compile/run:
     - Compile using
          make gerp
     - run executable with
          ./gerp DirectoryToIndex OutputFile


F. Architectural Overview

     Main.cpp is where the command line arguments are stored and used to make 
     an instance of the Gerp class and run its 2 main functions (index and 
     query) from. It makes sure that there is no usage error.

     The gerp class is where the main functions of the Gerp class are 
     implemented. It contains the functions that index the files in the 
     directory tree, take in user queries, and execute commands. It also 
     contains the functions that prompt the user for input and execute the 
     commands. It contains the functions that read the lines of the file, 
     parse the lines, and strip non-alphanumeric characters from the lines. 
     It contains the functions that open the file or die and abort the program.
     It contains the functions that print the locations of the word in the 
     files. Gerp contains a pointer to the root of the FS tree as a DirNode, 
     a pointer to the whole FS tree, a vector of File objects, and a pointer 
     to a HashTable object.

     The file class contains the functions that store the name of the file 
     and the path to the file as well as a vector of string representing 
     the lines of the file. It contains the functions that add a line to 
     the vector of lines and print the lines. It contains the functions 
     that return the number of lines in the file. These are stored within 
     a vector in the gerp class.

     The HashTable class contains a vector of linked lists that stores Value 
     structs. The Value struct stores the key (word), a set of wordVar structs,
     and a set of wordLocations. The wordVar struct stores the word variations
     (capitalization) and a set of wordLocations. The wordLocation struct 
     stores the line number and file index of the word.

G. Data Structures:

     The overall way we stored our indexed word data was by using our own 
     implementation of a hash table. This allowed us to have O(1) access to any 
     word we wished to query. This made sure that the bottleneck in our program 
     when searching was the size of the output not the size of the input. 
     The hashtable stored its series of buckets using an arraylist. The use of 
     an arraylist allowed the O(1) access of any index we wanted after using 
     our hash function.

     However, our HashTable in itself was composed of many more data 
     structures. Inside each bucket of our hashtable is a linked list of Value 
     structs. The use of a linked list was for collisions if two values mapped
     to the same bucket, and would be accomplished using chaining. 
     If there was a collision, the new value would get added to the back of 
     the linked list. We had to use a struct for our Value because we needed 
     to store the key. This allows us to find the right Value in each bucket 
     even if there are multiple. It made sense to use a linked list for 
     chaining because we would be adding to back and that is simply and O(1) 
     action. Additionally, we would have to traverse through the whole linked 
     list anyways so there is little advantage to using the arraylist.

     Our value struct includes the string for the key but it also contains 
     2 sets. It contains a set of word variations so it can differentiate 
     between the different capitalizations of words and still keep them in the 
     same place just in case the user wants to do an insensitive search. The 
     use of a set makes sure that there are new duplicated word variations and
     prevents any user error on our part in our implementation. The second set 
     is a set of pairs that contain the location of the word in the overall 
     index by using the form (int fileIndex, int lineNum). This allows us to 
     keep track of the location of the word in the file and the file it is in. 
     We used a set for this because we wanted to keep the locations in order 
     and we wanted to make sure that there were no duplicate locations, even if 
     the word appeared twice on the same line.

     The word variations mentioned earlier is a struct as well. It contains 
     the string of the word variation and a set of wordLocations. This is 
     similar to the Value struct in that it uses a set to keep track of the
     locations of the word. This is useful because it allows us to keep track 
     of the different capitalizations of the word and still keep them in the 
     same place just in case the user wants to do an insensitive search. The 
     set of wordLocations is the same as the set of wordLocations in the Value 
     struct, but it is specific to the word variation. The set of 
     wordLocations in the Value struct stops duplicates in insensitive 
     searches, but the set of wordLocations in the wordVar struct stops 
     duplicates in sensitive searches.

     We also used a vector of File objects to store the files and their 
     contents. This allowed us to easily access the files and their contents 
     when we needed to index them. We used a vector because we needed to be 
     able to add files to the vector and access the files and their contents by
     index. We also used a vector of strings to store the lines of the file. 
     This allowed us to easily access the lines of the file when we needed to
     index them. We used a vector because we needed to be able to add lines 
     to the vector and access the lines by index.

     We used a FSTree to store the directory tree. This allowed us to easily 
     traverse the directory tree and access the files and directories when we 
     needed to index them. We used a FSTree because it is a n-ary tree that 
     stores the file system and allows us to easily traverse the file system 
     and access the files and directories by node.


G. Testing:

     We used many different testing methods to test our program both as we 
     wrote it and as we tested it afterward for edge cases.  

     We tested our FSTreeTraversal by creating a test main function that 
     takes in command line arguments for a directory and inputted a variety of 
     directories to test that the output was as intended according to the spec. 
     We tested edge cases in this step by inputting a directory that does not 
     exist and directories within directories that contain no files.  

     We tested our stringProcessing function with the unit_testing framework. 
     We inputted a variety of strings with different formatting 
     (non-alphanumeric characters in front, back, middle) to test that the 
     output was as intended according to the spec. We tested edge cases in 
     this step containing only non-alphanumeric characters. Note, the 
     stringProcessing function is not included in our final submission as 
     the function was modified and added to our Gerp as stripNonAlphaNum.  
     We also did unit testing for the specific functions in our gerp class
     but we had to comment them out because the functions are private.

     Throughout writing our program, we implemented many cout statements 
     to check the current state of various variables and data structures. 
     For example, when we were creating our HashTable, we put cout statements 
     at the beginning of functions such as expand and traverseTree, so we could
     see when each function was called. In our insertWord function in the 
     HashTable class that was called for each word in each file, we put a 
     cout statement at the beginning of the function that printed the load 
     factor of the hash table, the number of buckets filled, and size of the 
     hashVector. This was used to check that the hash table was expanding when
     it was supposed to. After finding a bucket for the word, we had another
     cout that said whether the linked list at the bucket was currently 
     empty or not, indicating whether there was a collision or not. If we say
     the cout statement that said currVal was not nullptr, then we knew there 
     was a collision and that the word was being added to the back of the 
     linked list. We did a similar procedure for getBucket, where we put a 
     cout statement that was the bucket key and then the slot for the word 
     after it was calculated using the hash function. Our expand function also
     had many cout statements that printed when the function was called, 
     which we knew when it would happen at the beginning of indexing since 
     the HashTable was small and needed to expand many times until it got 
     large enough for the load factor to be less than 0.7 more often. The 
     cout statements printed the newCapacity of the table and the old size 
     of the table so we could see that it was expanding properly. Our 
     getWordLocation function took in both the string for a word and a 
     bool for whether the search was sensitive or not, so we printed both 
     of the inputted values when the function was called to check and then 
     cout statements in each of the if statements to see which one was being 
     executed.  

     To test our directory with our own input directories, we recreated the 
     “Foo” directory from the spec, which contained the files “a.cpp”, “b.cpp”,
     “c.cpp”, and another directory called “Bar” that contained files “x.cpp”,
     y.cpp, and “z.cpp”. Later on we also added a directory in “Foo” called 
     “Empty” that contained no files so we could test what the program would 
     do with an empty directory. Our cout statements with the few files let 
     us know that our program was operating properly and indexing correctly. 
     We outputted the file index and line numbers so we could check that the 
     program was indexing the files correctly. Note, these files are not 
     uploaded in a folder but were tested in a set of directories and 
     subdirectories. We also created a directory called single that 
     contained one file called “hello.txt” that contained the lyrics to 
     Frank Sinatra’s “My Way.” This simple test allowed us to discover that 
     when our function was expanding and rehashing. it was not rehashing 
     properly to put the buckets that already existed in the correct buckets 
     in the new table, so the duplicate instances of the same word were not 
     being stored in the same location as variations. Because of this, we 
     changed our rehashing function so that every bucket was properly moved 
     to the corresponding bucket in the new hashtable that was expanded. As 
     before, this file is not submitted in a directory but was originally in 
     a directory. Another test directory we made was called “test” that 
     contained files “test.txt” and “test2.txt” that both contained lines 
     containing the same word with different capitalization throughout the 
     line and then other lines with the same word in different capitalization
     so we could test that a single word with different capitalization 
     variation within one line and across multiple lines all mapped to the 
     same bucket.

     We also heavily referenced the the_gerp program to check how certain edge 
     cases were processed so that our program could perform the same way 
     exactly. For example, we checked how empty files were treated with the 
     demo program and passed into the demo strings such as strings that were 
     entirely non-alphanumeric characters to see what would happen. Then, we 
     made sure that our program performed the same way. We knew that the input 
     string needed to be stripped before it was passed through the functions 
     that search for the word because we passed string into the demo that were 
     just characters or string that we inputted with non-alphanumeric 
     characters in the front and back. We also checked how the demo program 
     handled the case where the user inputted a word that was not in any of 
     the files and made sure that our program performed the same way. Another 
     edge case we observed and tested in our testing was lines that contained 
     the same word multiple times with either the same or different 
     capitalization variations. The demo program only outputted the line once 
     when it contained multiple instances of the same word, so we made sure 
     that our program did the same and only printed the line once. Another 
     edge case we found while testing the reference program is that at the 
     end of an input file, the program should quit and should not repeatedly 
     ask for another query indefinitely. Initially, our program did not stop 
     asking for another query at the end of an input file but the demo file 
     did, so we had to modify our query conditional loop to make it so that 
     if the end of the cin input was reached, the program ended. We created 
     the test file called “inputNoQuit.txt” to test if the program stopped
     when the end of the file was reached when there was no “@q” or “@quit” 
     at the end of the input. There were other commands in this file to test 
     that the program would correctly output the proper output before reaching
     the end of file so there was something to diff test against. We 
     redirected the cin using the “<” operator to send in a file into 
     the cin stream. 

     We also created test functions that were meant to check the current 
     state of variables at various points throughout the indexing process. 
     In our HashTable class, we had a function called print_ValStruct that 
     printed the current state of the hashVector.This function printed a cout 
     statement when it was called, followed by the key of the Value *val 
     inputted, which is the key of that bucket. Then, the function would 
     traverse through all of the wordVar structs within that Value struct 
     and print the words in that bucket, and for every word in that bucket, 
     it would print the line number and file index of that word. There could 
     be multiple words in one bucket because of collisions and chaining.  
          
     At the end of completing our program, we did diff testing on the output 
     of files with all tinyData, smallGutenberg, mediumGutenberg, and 
     largeGutenberg directories. We created output files of our program and 
     the demo program with all of these directory and then sorting the output 
     files using the sort command and then diff the outputs. When there were 
     no differences in the files, we knew that our program was performing as 
     intended and was the same as the demo program. To allow the program to 
     print outputs without having the type in query commands each time, we 
     create the files “alphaInput.txt” and “mediumInput.txt” as files 
     containing query commands that we send into the program by redirecting
     the cin input using the “<” operator in the command line. Then, we 
     used the tinyData, smallGutenberg, mediumGutenberg, and largeGutenberg 
     directories on our program to create output files from our command file
     “mediumInput.txt” as cin. We also sent the same commands to the 
     reference program using the same set of directories to compare the 
     sorted diffs to ensure our program was functioning and outputting 
     correctly. 

     We used a variety of other input command files to send into our file to 
     set its output against the reference using diff testing and test the 
     query loop in our program. The file “singleInput.txt” was used to test 
     only a single search. The file “file_change.txt” was used to test the 
     query command @f that changed the file output file stream, as well as 
     “many_file_changes.txt” that was used to test multiple @f commands and 
     output redirections. The file “insen.txt” contained exclusively 
     insensitive searches to test the @i command in our program. The 
     “multi_input.txt” file was used to test many word queries on the 
     same line right after each other. The file “mixed.txt” was used to 
     test inputs that contained both insensitive and sensitive word searches. 
     The file “alphanumeric.txt” was a file with just non-alphanumeric 
     characters to test its function in our program. The file “notfound.txt” 
     was comprised of input that definitely would not be found and was meant 
     to test for outputs that returned no value.

     We also tested the time and space complexity of our program by using the 
     gerp_perf function on our own directories of small files as well as the 
     provided Gutenberg directories. As a result of this, we modified part of 
     our functions and indexing to improve the indexing time and minimize the 
     space used. 


H. Time Spent:

     We spent 40 hours on this project.



